---
title: "HW10"
author: "Jackson Passey"
date: "2023-03-30"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(invgamma)
library(scales)
set.seed(1)
```

# 1

## pre-usage of Gibbs

```{r}

bp <- c( 8, 12, 10, 14, 2, 0, 0) 

# prior values 
m <- 6 
v <- 32
a <- 3 
b <- 50 

# number of MCMC draws
J <- 10000

nnig.mcmc <- function(data,m,v,a,b,J){

    # data values
    ybar <- mean(data)
    s2 <- var(data)
    n <- length(data)
    
    # placeholder vector for MCMC iterations
    mu <- numeric()
    sigma2 <- numeric()
    
    # initial values
    mu[1] <- ybar
    sigma2[1] <- s2
    
    for (j in 2:J) {
      
      # update mu
      mstar <- (n*v*ybar + m*sigma2[j-1])/(n*v + sigma2[j-1])
      vstar <- (v*sigma2[j-1])/(n*v + sigma2[j-1])
      mu[j] <- rnorm(1, mstar, sqrt(vstar))
      
      # update sigma2
      astar <- (a+.5*n)
      bstar <- (b+.5*sum((data - mu[j])^2))
      sigma2[j] <- rinvgamma(1, astar, bstar)
      
    }
    out <- as.data.frame(cbind(mu,sigma2))
    colnames(out) <- c("mu","sigma2")
    return(out)
}
# Gibbs sampler
post <- nnig.mcmc(bp,m,v,a,b,J)

# create a trace plot
par(mfrow = c(2,1))
plot(post$mu, type = 'l', xlab = expression(mu), ylab = NA) # convergence is good
plot(post$sigma2, type = 'l', xlab = expression(sigma), ylab = NA) # convergence is good

```

After using the Gibbs Sampler, the prior values, and data, the trace plots suggest that the values for the posterior mean and variance do in fact converge. We see with the trace plots that random iterations of potential posterior parameters do "bounce around" and stick to a certain range of potential values. 

## a

```{r}

mean(post$mu > 0)

```

Given our prior knowledge and data, the posterior probability that $\mu > 0$ is `r mean(post$mu > 0)`

## b

```{r}

mean(post$sigma2 > 30)

```

Given our prior knowledge and data, the posterior probability that $\sigma^2 > 30$ is `r mean(post$sigma2 > 30)`

## c

```{r}

par(mfrow = c(1,1))

# normal
xs <- seq(m-3*sqrt(v),m+3*sqrt(v),length.out = 1001)

plot(density(post$mu), xlim = c(m-3*sqrt(v),m+3*sqrt(v)), 
     col = "blue", 
     main = expression(paste("Posterior Prior Distribution for ", mu)),
     xlab = expression(mu))
lines(xs, dnorm(x = xs, mean = m, sd = sqrt(v)), col = "red", type = "l")
legend("topright",legend = c("Prior","Posterior"),
       col = c("red","blue"), lwd = 2)
```

## d

``` {r}

xs2 <- seq(0,100,length.out = 1001)
plot(density(post$sigma2), xlim = c(0,100), ylim = c(0,.05), 
     col = "blue", 
     main = expression(paste("Posterior Prior Distribution for ", sigma^2)), 
     xlab = expression(sigma^2))
lines(xs2, dinvgamma(x = xs2, shape = a, rate = b), col = "red", type = "l")
legend("topright",legend = c("Prior","Posterior"),
       col = c("red","blue"), lwd = 2)

```

# 2

## pre-usage of Gibbs

```{r}

chol.urban <- c(197, 199, 214, 217, 222, 223, 227, 228, 234) 

# prior values 
m.urban <- 180 
v.urban <- 100
a.urban <- 2.1 
b.urban <- 480

# number of MCMC draws
J <- 10000

# Gibbs sampler
post.urban <- nnig.mcmc(chol.urban,m.urban,v.urban,a.urban,b.urban,J)

# create a trace plot
par(mfrow = c(2,1))
plot(post.urban$mu, type = 'l', xlab = expression(mu), ylab = NA) # convergence is good
plot(post.urban$sigma2, type = 'l', xlab = expression(sigma), ylab = NA) # convergence is good


```

After using the Gibbs Sampler, the prior values, and data, the trace plots suggest that the values for the posterior mean and variance do in fact converge. We see with the trace plots that random iterations of potential posterior parameters do "bounce around" and stick to a certain range of potential values. 

## a

```{r}

par(mfrow = c(1,2))

# normal

plot(density(post.urban$mu), 
     col = "gray", 
     main = expression(paste("Urban Posterior Distribution for ", mu)),
     xlab = expression(mu))

# sigma2

plot(density(post.urban$sigma2), 
     col = "gray", 
     main = expression(paste("Urban Posterior Distribution for ", sigma^2)), 
     xlab = expression(sigma^2))

```

## b

```{r}

mean(post.urban$sigma2)

```

Given our prior knowledge and data, the posterior expected value of $\sigma^2_{urban}$ is approximately `r mean(post.urban$sigma2)`

## c 

```{r}

mean(post.urban$mu)

```

Given our prior knowledge and data, the posterior expected value of $\mu_{urban}$ is approximately `r mean(post.urban$mu)`

## pre-usage of Gibbs for new priors

```{r}

# prior values 
m.urban2 <- 180 
v.urban2 <- 400

# number of MCMC draws
J <- 10000

# Gibbs sampler
post.urban2 <- nnig.mcmc(chol.urban,m.urban2,v.urban2,a.urban,b.urban,J)

# create a trace plot
par(mfrow = c(2,1))
plot(post.urban2$mu, type = 'l', xlab = expression(mu), ylab = NA) # convergence is good
plot(post.urban2$sigma2, type = 'l', xlab = expression(sigma), ylab = NA) # convergence is good

```

After using the Gibbs Sampler, the new prior values for $\mu$, and data, the trace plots suggest that the values for the posterior mean and variance do in fact converge. We see with the trace plots that random iterations of potential posterior parameters do "bounce around" and stick to a certain range of potential values. 

## d

```{r}

par(mfrow = c(1,1))

plot(density(post.urban$mu), 
     col = "gray", 
     main = expression(paste("Urban Posterior Distribution for ", mu)),
     xlab = expression(mu))

lines(density(post.urban2$mu), 
     col = "black")

legend("topright",legend = c("Part A","Part D"),
       col = c("gray","black"), lwd = 2)
```

# 3

## pre-usage of Gibbs

```{r}

chol.rural <- c(139, 142, 143, 144, 145, 148, 155, 162, 171, 181) 

# prior values 
m.rural <- 150 
v.rural <- 500
a.rural <- 3
b.rural <- 180

# number of MCMC draws
J <- 10000

# Gibbs sampler
post.rural <- nnig.mcmc(chol.rural,m.rural,v.rural,a.rural,b.rural,J)

# create a trace plot
par(mfrow = c(2,1))
plot(post.rural$mu, type = 'l', xlab = expression(mu), ylab = NA) # convergence is good
plot(post.rural$sigma2, type = 'l', xlab = expression(sigma), ylab = NA) # convergence is good

```

After using the Gibbs Sampler, the prior values, and data, the trace plots suggest that the values for the posterior mean and variance do in fact converge. We see with the trace plots that random iterations of potential posterior parameters do "bounce around" and stick to a certain range of potential values. 

# Plot

```{r}

par(mfrow = c(1,1))

plot(density(post.rural$mu), 
     col = "black", 
     main = expression(paste("Rural Posterior Distribution for ", mu)),
     xlab = expression(mu))

```

# 4

Because we have used the Gibbs sampling method for each of the individual distributions, we won't need to complete a Gibbs sampling procedure for the difference. It would be hard to derive a prior for the difference and run the sampler. We can use a trace plot to show convergence for urban-rural posterior parameters

```{r}

# create a trace plot
par(mfrow = c(2,1))
plot(post.urban$mu - post.rural$mu, type = 'l', xlab = expression(mu), ylab = NA) 
plot(post.urban$sigma2 / post.rural$sigma2, type = 'l', xlab = expression(sigma), ylab = NA) 
```

We can see that the parameter difference between urban and rural does in fact converge, so we can proceed just as we assumed. 

## a

```{r}

# dist. of urban - rural mu 
plot(density(post.urban$mu - post.rural$mu), 
     main = expression(paste("Urban-Rural Posterior Distribution for ", mu)),
     xlab = expression(mu[urban]-mu[rural]))

quantile(post.urban$mu - post.rural$mu, c(.025,.975))

```

Given our prior knowledge and data, there is a .95 probability that the true difference in means between urban and rural cholesterol levels is between `r  quantile(post.urban$mu - post.rural$mu, c(.025,.975))`. We can thus conclude that there is no real significant difference between the two means as the interval includes 0. 

## b

```{r}

plot(density(post.urban$sigma2 / post.rural$sigma2), 
     xlim = c(0,15),
     main = expression(paste("Urban-Rural Posterior Distribution for ", sigma^2)),
     xlab = expression(sigma[urban]^2-sigma[rural]^2))

quantile(post.urban$sigma2 / post.rural$sigma2, c(.025,.975))

```

Given our prior knowledge and data, there is a .95 probability that the true ratio of variances between urban and rural cholesterol levels is between `r  quantile(post.urban$sigma2 / post.rural$sigma2, c(.025,.975))`. We can thus conclude that there is no real significant difference between the two variances as the interval includes 1. 

# 5

Since we already have our Gibbs-sampling distributions, we don't need to create any for this problem.

```{r}

post.pred.urban <- rnorm(nrow(post.urban), 
                         mean = post.urban$mu,
                         sd = sqrt(post.urban$sigma2))
post.pred.rural <- rnorm(nrow(post.rural), 
                         mean = post.rural$mu,
                         sd = sqrt(post.rural$sigma2))

hist(post.pred.rural, breaks = 30, col = alpha("orange1", .75), 
     main = expression(paste("Urban and Rural Posterior Predictive Distribution for ", mu)),
     xlab = "Predictions")
hist(post.pred.urban, breaks = 30, col = alpha("lightblue", .75), add = T)
legend("topright",legend = c("Urban","Rural"),
       col = c("lightblue","orange1"), pch = 15)

post.pred.difs <- post.pred.urban - post.pred.rural

# interval
quantile(post.pred.difs, c(.025,.975))

```

Given our prior knowledge and data, the posterior predictive distribution noted that there is a 95% probability that a future observation for an urban citizen's cholesterol level and a rural citizen's cholesterol level will have a difference of between `r quantile(post.pred.difs, c(.025,.975))`. Since this interval is in fact a prediction interval, we can look back from previous coursework that a prediction interval is larger in width than confidence and credible intervals. Because we are doing inference on a singular prediction rather than parameters/means, we have more variability to account for. Thus, we find that this interval is much larger than the credible interval in part 4a. 
